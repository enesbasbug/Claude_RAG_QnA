{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --quiet datasets=='2.18.0' anthropic=='0.21.3' voyageai=='0.2.1' qdrant-client=='1.7.1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Building a RAG Agent Utilizing Claude 3 Opus\n",
    "\n",
    "In our project, we aim to create an efficient RAG (Retrieval-Augmented Generation) agent by integrating several components:\n",
    "\n",
    "- **LangChain**: To manage the workflow.\n",
    "- **Voyage AI**: For knowledge embeddings.\n",
    "- **Qdrant Database**: Serving as our primary data storage.\n",
    "- **Claude 3 Opus**: As our Language Model.\n",
    "- **Hugging Face Dataset**: For our dataset needs.\n",
    "\n",
    "### Quick Intro\n",
    "1. We'll be using the AI ArXiv dataset from Hugging Face, specifically the prechunked version: [Link](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks)\n",
    "    - The full version is available [Here](https://huggingface.co/datasets/jamescalam/ai-arxiv2)  \n",
    "\n",
    "&nbsp;\n",
    "2. Voyage Setup\n",
    "\n",
    "&nbsp;\n",
    "3. Qdrant DB Setup\n",
    "\n",
    "&nbsp;\n",
    "4. Anthropich Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### 1- Dataset\n",
    "\n",
    "We will be utilising version 2 of the AI ArXiv dataset from Hugging Face. Our chosen dataset is already pre-chunked for convenience. However, there is also an option to work with the raw/plain version of the dataset. (Refer to the links mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train[:10000]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2- Voyage  Setup\n",
    "\n",
    "Regarding embeddings, we'll use VoyageEmbeddings, leveraging the embedding models provided by Voyage AI. An API key is necessary for this process. \n",
    "\n",
    "Initially, we'll establish a connection with Voyage AI and then create an embed object specifically for handling embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import VoyageEmbeddings\n",
    "\n",
    "load_dotenv()  # This loads the variables from .env into the environment\n",
    "\n",
    "voyage_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "embed = VoyageEmbeddings(\n",
    "    voyage_api_key=voyage_key, model=\"voyage-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 3- Qdrant DB Setup\n",
    "\n",
    "We will use a vector database to store and query our embeddings. For this purpose, Qdrant is our chosen platform, which also necessitates obtaining a free API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "\n",
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=qdrant_url, \n",
    "    api_key=qdrant_api_key\n",
    ")\n",
    "collection = \"rag_with_claude\"\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection,\n",
    "    vectors_config=models.VectorParams(size=1024, distance=models.Distance.DOT),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "this code processes a dataset in batches, generates embeddings for each batch, and then stores these embeddings along with their metadata in a Qdrant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import uuid\n",
    "\n",
    "\"\"\"\n",
    "- tqdm; imported for showing a progress bar during loops.\n",
    "- uuid; used for generating unique identifiers.\n",
    "\"\"\"\n",
    "\n",
    "data = dataset.to_pandas()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    # get metadata to store in Qdrant\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    \n",
    "    points = [{'id': id, 'vector': embed, 'payload': meta} for id, embed, meta in zip(ids, embeds, metadata)]\n",
    "\n",
    "    operation_info = qdrant_client.upsert(\n",
    "        collection_name=collection,\n",
    "        points=points\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "def arxiv_search(query: str) -> str:\n",
    "    \"\"\"For inquiries about artificial intelligence, machine learning, data science, \n",
    "    or other technical fields where arXiv papers might offer relevant \n",
    "    information and answers, consider using this tool.\n",
    "    \"\"\"\n",
    "    # create query vector\n",
    "    query_vector = embed.embed_query(query)\n",
    "\n",
    "    # perform search in Qdrant\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=query_vector,\n",
    "        limit=5,\n",
    "        append_payload=True\n",
    "    )\n",
    "\n",
    "    return search_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = arxiv_search(\"Do you know what claude ai is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results = []\n",
    "for idx, result in enumerate(results, start=1):\n",
    "    source = result.payload['source']\n",
    "    title = result.payload['title']\n",
    "    text = result.payload['text']\n",
    "    formatted_results.append(f\"Result-{idx}\\nSource:\\n{source}\\nTitle:\\n{title}\\nText:\\n{text}\")\n",
    "\n",
    "full_text = '\\n\\n'.join(formatted_results)\n",
    "print(full_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 4- Anthropic - Claude AI\n",
    "- Next we initialize our connection to Anthropic, for this we need an Anthropic API key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "client = anthropic.Client(api_key=anthropic_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "- #### RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_user_query(query):\n",
    "\n",
    "    results = arxiv_search(query)\n",
    "    \n",
    "    formatted_results = []\n",
    "    for idx, result in enumerate(results, start=1):\n",
    "        source = result.payload['source']\n",
    "        title = result.payload['title']\n",
    "        text = result.payload['text']\n",
    "        formatted_results.append(f\"Result-{idx}\\nSource:\\n{source}\\nTitle:\\n{title}\\nText:\\n{text}\")\n",
    "\n",
    "    search_result = '\\n\\n'.join(formatted_results)\n",
    "\n",
    "    response = client.messages.create(\n",
    "    model = \"claude-3-opus-20240229\",\n",
    "    max_tokens = 1024,\n",
    "    system = \"\"\"\n",
    "    Your task is to provide informed and accurate answers\n",
    "    to technical inquiries in artificial intelligence, machine learning and data science, \n",
    "    trained extensively on arXiv papers.\"\"\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Answer this user query: \" + query + \" with the following context: \\n \" + search_result\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "    return (response.content[0].text), search_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is llama better than GPT?\"\n",
    "response, search_result = handle_user_query(query)\n",
    "print(f\"Response:\\n {response}\")\n",
    "print(\"---\"*30 + \"\\n\"*2)\n",
    "print(f\"Search Result:\\n {search_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
